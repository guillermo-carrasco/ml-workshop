{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests and automated parameter searching\n",
    "\n",
    "Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n",
    "\n",
    "Even today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the random forest as an example.\n",
    "\n",
    "The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters.\n",
    "\n",
    "We build a random forest model similarly to how we built a decision tree in `scikit-learn` - this time using the `RandomForestRegressor` class instead of `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillermo/.anaconda/envs/ml-workshop/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Don't modify\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df = pd.read_csv('../data/housing/train.csv')\n",
    "\n",
    "features = [\n",
    "    'LotArea',\n",
    "    'YearBuilt',\n",
    "    '1stFlrSF',\n",
    "    '2ndFlrSF',\n",
    "    'FullBath',\n",
    "    'BedroomAbvGr',\n",
    "    'TotRmsAbvGrd'\n",
    "]\n",
    "target = 'SalePrice'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same procedure than before, fit the `RandomForest` and calculate MAE. The best we got with a decision tree was ~27282.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for the raw Random Forest is 22136.903517286366\n"
     ]
    }
   ],
   "source": [
    "# Fit the random forest\n",
    "model = RandomForestRegressor()\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# Predict on val_X and calculate MAE with val_y\n",
    "predictions = model.predict(val_X)\n",
    "mae = mean_absolute_error(val_y, predictions)\n",
    "print(f'MAE for the raw Random Forest is {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is already lower, with no need of parameter tuning. But we can do better. Observe how many parameters does a `RandomForest` have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the parameters that configure the model are called **hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated hyperparameter optimization\n",
    "\n",
    "### Grid Search\n",
    "we could try to do something similar to what we did when we experimented with the amount of leaves, but for more parameters. As you may be imagining, there is a better way to do this than with for loops. There is a technique called **Grid Search**. When running a grid search, you will basically specify the number of parameters that you want to try, and the algorithm will fit a model _for every possible combination_. This means that you need to be careful, or the computational cost will grow exponentially. \n",
    "\n",
    "### Cross Validation\n",
    "If we only use one validation set to tune our hyperparameters, we still run the risk of overfitting. This is because we are performing lots of tests on the same subset of the data. One solution to this is **cross validation**. In cross validation, we split the training data into _K_ subsets called _folds_. The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "* A model is trained using  of the folds as training data\n",
    "* the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "![Cross Validation](../data/misc/grid_search_cross_validation.png)\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data and helps a lot in generalizing a model\n",
    "\n",
    "`scikit-learn` provides a method that combines both **Grid Search** and **Cross Validation**. Called, surprisingly **GridSearchCV**.\n",
    "\n",
    "Let's try to tune several of the random forest parameters, I will provide the list of parameters to tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [None, 500, 700, 800, 900],\n",
    "    'max_leaf_nodes': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'n_estimators': [30, 50, 60],\n",
    "    'criterion': ['mae']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  3.6min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "# Define a grid search with a RandomForestRegressor as estimatos and the previous list of parameters\n",
    "grid = GridSearchCV(RandomForestRegressor(), params, verbose=3)\n",
    "\n",
    "with parallel_backend('loky', n_jobs=-1): # This will make use of all your processors\n",
    "    grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'mae',\n",
       " 'max_depth': 700,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'n_estimators': 60}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the best set of parameters from the grid\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for the best tuned Random Forest is 22073.400273972602\n"
     ]
    }
   ],
   "source": [
    "# Get the best estimator\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Train it with train data\n",
    "best_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions on validation set\n",
    "predictions = best_model.predict(val_X)\n",
    "\n",
    "# And calculate the MAE\n",
    "mae = mean_absolute_error(val_y, predictions)\n",
    "\n",
    "print(f'MAE for the best tuned Random Forest is {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model has improved even further. It doesn't look like much, but given the very small amount of data for this problem, is quite okay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
